---
title: "Back-End OCTAD Portal"
output:
  html_document:
    toc: true
    theme: united
---
This is for the back end demo, which only uses 5000 random samples from the full data set.  

Differences between running demo and actual dataset:  
For actual,  

1) Use "dz.expr.log2.readCounts.RData" instead of "dz.expr.log2.readCounts.demo.RData"    
2) Do not run "phenoDF = phenoDF %>% filter(sample.id %in% colnames(dz.expr.log2.read.count.demo))"  
3) Use "normal_counts = dz.expr.log2.read.count[,normal_id]  
case_counts = dz.expr.log2.read.count[,case_id]"  
instead of     
"normal_counts = dz.expr.log2.read.count.demo[,normal_id]  
case_counts = dz.expr.log2.read.count.demo[,case_id]"  
4) Use default n_topGenes = 10000 instead of 2000.

***
Start Time of analysis.  Takes about ~15min to run demo from start to the heatmap showing potential drugs for reversal.  The last segment, Drug Enrichment Analysis, may take more than 30min to run.
```{r}
Sys.time()
``` 
***
##File Locations##
***  
Set project's overarching folder that contains code, pipelinedata and output folders
```{r}
setwd('~/Documents/GitHub/OCTAD v180627/Clone 6 - Anita/') 
```

Indicate your pipelineDataFolder.  
Have the following **4** files in your pipelineDataFolder:

* integrated.OCTAD.basic.sample.meta.csv [phenotype data]
* dz.expr.log2.readCounts.demo.RData (demo) or dz.expr.log2.readCounts.RData (full set) [expression data]    
* gencode.v23.annotation.gene.probeMap.csv [features data]   
* Gene_info_hs.csv [features data2]    
```{r, results='hide'}
pipelineDataFolder = 'Data/'
dir(pipelineDataFolder)
#check to make sure you have these files
```
***

Indicate your WebPortalCodeFolder    
Have the following **8** scripts in your WebPortalCodeFolder:

* DE_core_functions.R  
* Diff_Exp.R  
* drug_enrichment_analysis.R  
* drugs_core_functions.R  
* gene_enrichment_analysis.R  
* runRGES_dz_arrangeMax.R  
* tnse_2d_5000varGenes.RData  
* visualize_drug_hits.R  
```{r}
WebPortalCodeFolder = 'Web Portal Codes/'
dir(WebPortalCodeFolder)
#check to make sure you have thee files
```
***

Indicate your outputFolder. Make a new outputFolder for each run to prevent overwriting older files.  
From one run, you will generate at least **15** files. 

* all_lincs_score.csv
* case_control_map.pdf
* case_normal_corMatrix.csv
* case_normal_median_cor.csv
* computedEmpGenes.csv
* correlation graph.pdf
* DE_genes.csv
* dz_down_sig_genes_enriched.csv
* dz_sig_used.csv
* dz_signature.csv
* dz_up_sig_genes_enriched.csv
* highExpGenes.csv
* lincs_reverse_expression.pdf
* res_geneinfo.csv
* sRGES.csv
```{r}
outputFolder = 'DEMO output2/'
if (!dir.exists(outputFolder)) {
  dir.create(outputFolder)
}
```
***  

Load packages
```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
#BiocInstaller::biocLite('RUVSeq')
library ("RUVSeq")
library("GSVA")
```
***  
Load the dataframes that contains your case and normal reference tissues.  
DO NOT VIEW these dataframes as it is very large. Use the colnames, rownames, and dim functions. Otherwise, you may crashed.
```{r}
load(paste0(pipelineDataFolder,'dz.expr.log2.readCounts.demo.RData')) 
#File dz.expr.log2.readCounts.demo.RData will give you mRNA counts. 
#the expression dataframe table is called dz.expr.log2.read.count.demo 

phenoDF = read.csv(paste0(pipelineDataFolder,'integrated.OCTAD.basic.sample.meta.csv'),stringsAsFactors = F)
#File integrated.OCTAD.basic.sample.meta.csv consist of sample ID, cancer location, and data source, etc.  
#the phenotype dataframe is called phenoDF 

phenoDF = phenoDF %>% filter(sample.id %in% colnames(dz.expr.log2.read.count.demo))
#since we are dealing with demo data, there are fewer number of samples, so we need to filter out samples not within this demo.
```
***

##Explore the Phenotype data##
The phenoDF table contains thousands of samples from multiple databases. (Demo only has 5000 random samples). Run the following code to explore the phenoDF table. 
```{r}
dim(phenoDF)
head(phenoDF)
#table(phenoDF$sample.type) 
#table(phenoDF$data.source)
table(phenoDF$data.source, phenoDF$sample.type)
#DF stands for dataframe
```
***  

##Select Cases##    

Select your cancer of interest.  You can filter through the available columns in phenoDF to make the phenoDF_case table. 
The following example will only contain data rows having "Brain Lower and Grade Glioma" for cancer and "Primary" for sample.type.  
```{r}
phenoDF_case <- phenoDF %>% 
  filter(cancer == 'Brain Lower Grade Glioma',
         sample.type %in% c('primary'))
dim (phenoDF_case)
head (phenoDF_case)
```
***  

Create a list with sample IDs for the cancer of interest (case). 
```{r}
case_id <- phenoDF_case$sample.id
str(case_id)
```
Results: **`r nrow(phenoDF_case)`** case IDs.  
***

##Select Reference Control Samples##  
***

Identify the sample.id of normal tissues in the phenoDF. They should be from the GTEX database and contain "GTEX..." in the start of the sample ID name.
```{r}
normal_id = (phenoDF %>% filter(data.source == 'GTEX'))$sample.id
str(normal_id)

#normal_id = (phenoDF %>% filter(sample.type == 'normal'))$sample.id
#str(normal_id)
```
***

Run DE_core_functions.R, which contains the computeRefTissue function.
```{r}
source(paste0(WebPortalCodeFolder,'DE_core_functions.R'))
#computes ref tissue given case_id strings, normal_id strings, and the df that contains them both as colnames
#expSet is a counts dataframe that contains both case and normal counts. 
```
***

Define normal and case counts. Combine the normal and case counts by generating a table called dz_expr containing columns of normal and case sample IDs vs rows of genes. 
```{r}
normal_counts = dz.expr.log2.read.count.demo[,normal_id]
case_counts = dz.expr.log2.read.count.demo[,case_id]

(row.names(case_counts) == row.names(normal_counts)) %>% sum()
#quick check to see that all genes match up before combining  

dz_expr = cbind(normal_counts, case_counts)

dim(dz_expr)
```
Results: **`r nrow(dz_expr)`** rows of genes vs **`r ncol(dz_expr)`** total columns of combined **`r nrow(phenoDF %>% filter(data.source == 'GTEX'))`** normal and **`r nrow(phenoDF_case)`** case samples  

***

Define your control ID--the control samples you are comparing your case with. The computeRefTussue function will match similar normal samples to your chosen cancer.  This function is based on mRNA data of high varying expression across all tissues.  
You will generate **50** control samples, unless you state a different control size.
```{r}
control_id <- computeRefTissue(case_id = case_id,
                               normal_id = normal_id,
                               expSet = dz_expr,
                               control_size = 50) 
head(control_id)
#this function will also generate a csv called case_normal_median_cor.csv that includes the median correlation of each normal sample.
```
***

Check the tissue location of where the control samples are from.
```{r}
(phenoDF %>% filter(sample.id %in% control_id) %>% select(biopsy.site) %>% table())
```
Since my chosen cancer is a type of brain cancer, the code chose 50 control samples from the brain. If I want, I can exclude certain samples.
***

##Visualization of Case Vs. Reference Samples##  
***

To see how similar the controls are to the case samples, get the correlations of mRNA expressions between each of the normal samples and the set of cases. The list is in descending order.
```{r}
normal_median_cor = read.csv(paste0(outputFolder,'/case_normal_median_cor.csv'),stringsAsFactors = F)
normal_median_cor <- normal_median_cor %>% arrange(desc(cor))

normal_median_cor <-left_join(normal_median_cor, 
  phenoDF %>% select(sample.id, biopsy.site), 
  by = "sample.id")

head(normal_median_cor)
```
***

Goal is to sort the correlation order based on tissue type.  
***

I) Find median correlation for each primary site
```{r}
(reference_tissue_rank <- normal_median_cor %>% 
  group_by(biopsy.site) %>% summarise(medianCor = median(cor)) %>% 
  ungroup())
```

II) Sort the median correlation of the different tissues from high to low
```{r}
(top_refs = reference_tissue_rank %>% arrange(desc(medianCor)))
#top_refs <-  reference_tissue_rank[1:10, 1]
```

III) Sort tissue from high to low correlation. Top ten shown.
```{r}
normal_median_cor$ref <- factor(normal_median_cor$biopsy.site, levels = top_refs$biopsy.site)
levels(normal_median_cor$ref)[1:10]
```

IV) Graph tissue type vs mRNA correlation
```{r, fig.height=9, fig.align= "center"}

p <- ggplot(normal_median_cor, aes(ref, cor))
p +   geom_boxplot(color='grey', notch=F, outlier.shape = NA) + 
  geom_jitter(aes(alpha=1/100), show.legend = F) + 
  theme_bw() + 
  ylab("correlation") + 
  xlab("") + 
  labs (title='Correlation between Case Samples and Reference Tissue',caption='OCTAD')+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 11),
  axis.text.y = element_text(size = 11), axis.title = element_text(size = 11), 
  plot.margin = margin(l=0))
```
***

```{r}
ggsave('correlation graph.pdf', paper='a4r', path=outputFolder)
```

##Visualize on Cancer Map##
This is optional.
```{r, fig.height=10, fig.align= "center"}

#tsne = read.csv('Data/tsne_2d_5000varGenes.csv', stringsAsFactors = F)

#pheno_id_cancer <- phenoDF %>% select (sample.id, cancer)
#tsne <- left_join (tsne, pheno_id_cancer, by = "sample.id")

#make a new column called type to state case, control, or others
#tsne$type <- "others"
#tsne$type[tsne$sample.id %in% case_id] <- "case"
#tsne$type[tsne$sample.id %in% control_id] <- "control"

#plot 
#(p2 <- ggplot(tsne, aes(X, Y, color = type)) + geom_point(alpha = 0.4)+
#    labs(title = paste ('TNSE PLOT'), x= 'TSNE Dim1', y='TSNE Dim2', caption="OCTAD"))
```

```{r}
#ggsave("case_control_map.pdf", path=outputFolder)
```

##Optional: Selecting Another Set of Reference Samples##  
***
```{r}
#2a. select normals based on the top site computed e.g. if brain was top site select all brain as controls
#2b. select normals based on the top primary site
  #note there are subtypes of which brain sites are there
  #instead of all brain sites we can select the part with highest correlation
    #e.g. cortex, medulla etc...
#2c. change the graph to plot more specific sites mentioned in 2b. 

#rm(p,normal_median_cor,phenoDF,phenoDF_case,reference_tissue_rank,top_refs) before selecting another set.
```
***

##Run Differential Expressions##  
***

Variables needed:  

* case_id : samples IDs of case tissues in a character vector
* control_id : samples IDs of control tissues in a character vector
* dz_expr : dataframe that contains both case_id and control_id in its column names, 
* outputFolder : folder to place output files

***
Here, you will normalize samples for batch effects with package "RUVSeq". You can set it off by changing set parameters script.   
Set inital parameters for normalization. Below are default settings. Note down parameters that you changed from default
Keep a record of these settings if you're doing multiple runs.
You can set normalize_samples = F to make it run faster...but at risk of batch effects.
```{r}
normalize_samples = F
k = 1
n_topGenes = 2000
#this demo sample set does not have too much genes. The usual default for n_topGenes = 10000.
#set this higher if you think there are more significant genes
DE_method = 'edgeR' 
#Other choice is 'limma' for DE_method.
```
***

Read csv with gene information and read Diff_Exp.R script.  
This will output 3 files: "computedEmpGenes.csv", "DE_genes.csv", and  "highExpGenes.csv".  
```{r, message=FALSE, warning=FALSE}
gene_info <- read.csv(paste0(pipelineDataFolder,'/gene_info_hs.csv'),stringsAsFactors = F)
source(paste0(WebPortalCodeFolder,'Diff_Exp.R'))
```
***

##Disease Signature##  
***  

Get the disease signature.
```{r}
dz_signature <- res %>% filter(padj < 0.001, abs(log2FoldChange)>1)
#filter out res df to get dz sigs
#Term res refers to the results table that is generated from the Diff_Exp. R script.
dim(dz_signature)
head(dz_signature)
```
***

Read the Ensembl genome database. Combine the res table with the Ensembl table to generate a differential expression results table with gene information.
```{r}
ensembl_info <- read.csv(paste0(pipelineDataFolder,'/gencode.v23.annotation.gene.probeMap.csv'),
                         stringsAsFactors = F)
res_geneinfo = res %>% left_join(ensembl_info %>% select(gene,ensembl),by=c('identifier'='ensembl')) %>% 
  left_join(gene_info,by=c('gene'='Symbol'))
#since the gene_identifier in the res table are ensembl, we need to load this metadata to join them
#the RGES data requires gene symbol
```
***

Output the res_geneinfo.csv file.
```{r}
write.csv(res_geneinfo,file=paste0(outputFolder,'res_geneinfo.csv'))
```
***

Generate table for disease signature
```{r}
dz_signature <- dz_signature %>% left_join(ensembl_info %>% select(gene,ensembl,chrom,strand),
                                           by=c('identifier'='ensembl'))
dim(dz_signature)
head(dz_signature)
```
***

Capitalize the genes in the disease signature
```{r}
#the column name may differ between different metadata but you need Symbol to join with RGES
dz_signature$Symbol <- toupper(dz_signature$gene)
str(dz_signature$Symbol)
```
***

Generate "dz_signature.csv" file.
```{r}
write.csv(dz_signature, paste0(outputFolder, "/dz_signature.csv"),row.names = F)
```
***

##Affected Pathways and Potential Drugs for Reversal##  
***
Find potential drugs that may reverse disease signature.  Takes at least 10mins to run following chunk. Following scripts will output "all_lincs_score.csv"" and "sRGES.csv".  
File all_lincs_score.csv has the RGES (reversal gene expression score), which is needed for the gene_enrichment_analysis script later.    
File sREGS.csv is the summarized RGES.
```{r}
source(paste0(WebPortalCodeFolder,'drugs_core_functions.R'))
#functions needed to run RGES
source(paste0(WebPortalCodeFolder,'runRGES_dz_arrangeMax.R'))
```
***

See which pathways are affected by running following code. This results the up-regulated pathways as "dz_up_sig_genes_enriched.csv" and the down-regulated pathways as "dz_dn_sig_genes_enriched.csv". 
```{r}
source(paste0(WebPortalCodeFolder,'gene_enrichment_analysis.R'))
```
***

Ouput the "lincs_reverse_expression.pdf' containing a heatmap showing drug hits and their effects on mRNA expression.
```{r, message=FALSE, warning=FALSE}
source(paste0(WebPortalCodeFolder,'visualize_drug_hits.R'))
```
***

##Drug Enrichment Analysis##
Additional files are needed here, depending on target_type.  
Each of the target_type will create a pdf and a csv file. 

* cmpd_sets_chembl_target.RData  
* cmpd_sets_mesh.RData  
* cmpd_sets_meshes.RData  
* cmpd_sets_sea_targets.RData    

The following three chunks are either both FDA-approved and not, FDA-approved only, or not FDA-approved. These chunks can take over 30min to run.

***  
1) for all results--both FDA-approved and not.  
```{r}
Sys.time()
source(paste0(WebPortalCodeFolder,'drugs_core_functions.R'))
targets = c('chembl_targets','mesh','meshes','sea_targets')

for (target_type in targets){
  load(paste0(pipelineDataFolder,"cmpd_sets_", target_type, ".RData"))
  cmpdSets = cmpd_sets$cmpd.sets
  sRGES = read.csv(paste0(outputFolder,'/sRGES.csv'),stringsAsFactors = F)
  
  gsea_p = drug_enrichment(sRGES=sRGES,target_type=target_type)
  write.csv(gsea_p, paste0(outputFolder, "/enriched_sRGES_all", target_type, ".csv"))
  top_target = gsea_p$target[1]
  sRGES$rank = rank(-sRGES$sRGES)
  target_drugs_score = sRGES$rank[sRGES$pert_iname %in% cmpdSets[[top_target]]]
  pdf(paste0(outputFolder, "/top_enriched_sRGES_all_", target_type, ".pdf"))
  limma::barcodeplot(sRGES$sRGES, target_drugs_score, main = top_target, xlab = "sRGES")
  dev.off()
}
```

2) OPTIONAL for FDA-approved only 
Can Modify within the forloop
-filter drugs for sRGES that you're interested in 
```{r}
Sys.time()
source(paste0(WebPortalCodeFolder,'drugs_core_functions.R'))
targets = c('chembl_targets','mesh','meshes','sea_targets')

for (target_type in targets){
  load(paste0(pipelineDataFolder,"cmpd_sets_", target_type, ".RData"))
  cmpdSets = cmpd_sets$cmpd.sets
  fda_only <- read.csv(paste0(pipelineDataFolder,'LINCS_RGES/repurposing_drugs_20170327.txt'),skip=9,
                     sep='\t')
  sRGES = read.csv(paste0(outputFolder,'/sRGES.csv'),stringsAsFactors = F)
  sRGES = sRGES %>% filter (pert_iname %in% fda_only$pert_iname) %>% 
    left_join(fda_only,by='pert_iname')

  gsea_p = drug_enrichment(sRGES=sRGES,target_type=target_type)
  write.csv(gsea_p, paste0(outputFolder, "/enriched_sRGES_fda_only_", target_type, ".csv"))
  top_target = gsea_p$target[1]
  sRGES$rank = rank(-sRGES$sRGES)
  target_drugs_score = sRGES$rank[sRGES$pert_iname %in% cmpdSets[[top_target]]]
  pdf(paste0(outputFolder, "/top_enriched_sRGES_fda_only_", target_type, ".pdf"))
  limma::barcodeplot(sRGES$sRGES, target_drugs_score, main = top_target, xlab = "sRGES")
  dev.off()
}
```
*** 

3) OPTIONAL for non-FDA-approved
```{r}
Sys.time()
source(paste0(WebPortalCodeFolder,'drugs_core_functions.R'))
targets = c('chembl_targets','mesh','meshes','sea_targets')

for (target_type in targets){
  load(paste0(pipelineDataFolder,"cmpd_sets_", target_type, ".RData"))
  cmpdSets = cmpd_sets$cmpd.sets
  nonfda <- read.csv(paste0(pipelineDataFolder,'LINCS_RGES/repurposing_drugs_20170327.txt'),skip=9,
                     sep='\t')
  sRGES = read.csv(paste0(outputFolder,'/sRGES.csv'),stringsAsFactors = F)
  sRGES = sRGES %>% filter (!(pert_iname %in% fda_only$pert_iname))

  gsea_p = drug_enrichment(sRGES=sRGES,target_type=target_type)
  write.csv(gsea_p, paste0(outputFolder, "/enriched_sRGES_nonfda_", target_type, ".csv"))
  top_target = gsea_p$target[1]
  sRGES$rank = rank(-sRGES$sRGES)
  target_drugs_score = sRGES$rank[sRGES$pert_iname %in% cmpdSets[[top_target]]]
  pdf(paste0(outputFolder, "/top_enriched_sRGES_nonfda_", target_type, ".pdf"))
  limma::barcodeplot(sRGES$sRGES, target_drugs_score, main = top_target, xlab = "sRGES")
  dev.off()
}
```
***  

##Platform and packages used in analysis##
```{r}
library(devtools)
session_info()
```
***

End Time
```{r}
Sys.time()
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

```

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

