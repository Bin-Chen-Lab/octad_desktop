---
title: "Back-End OCTAD Portal"
output:
  html_document:
    toc: true
    theme: united
---

***
Start Time of analysis.  Takes about ~15min to run demo from start to the heatmap showing potential drugs for reversal.  The last segment, Drug Enrichment Analysis, may take more than 30min to run.
```{r Folder Setup}
setwd('~/octad_desktop/LargeScaleAnalysis/') 
pipelineDataFolder = '../Data/'
dir(pipelineDataFolder)

CodeFolder = '../Code/'
dir(CodeFolder)
outputFolder = 'Workflow-BRCA-LumA-Adjacent_noNorm/'
# outputFolder = paste0('Workflow-HCC-A-',Sys.Date(),'/')
if (!dir.exists(outputFolder)) {
  dir.create(outputFolder)
}
``` 
***
##File Locations##
***  
Set project's overarching folder that contains code, pipelinedata and output folders
Notebook should be at the root of this folder

Indicate your pipelineDataFolder.  
Have the following **4** files in your pipelineDataFolder:

* integrated.OCTAD.basic.sample.meta.csv [phenotype data]
* dz.expr.log2.readCounts.demo.RData (demo) or dz.expr.log2.readCounts.RData (full set) [expression data]    
* gencode.v23.annotation.gene.probeMap.csv [features data]   
* Gene_info_hs.csv [features data2]    

Indicate your CodeFolder    
Have the following **8** scripts in your CodeFolder:

* DE_core_functions.R  
* Diff_Exp.R  
* drug_enrichment_analysis.R  
* drugs_core_functions.R  
* gene_enrichment_analysis.R  
* runRGES_dz_arrangeMax.R  
* tnse_2d_5000varGenes.RData  
* visualize_drug_hits.R  

***

Indicate your outputFolder. Make a new outputFolder for each run to prevent overwriting older files.  
From one run, you will generate at least **15** files. 

* all_lincs_score.csv
* case_control_map.pdf
* case_normal_corMatrix.csv
* case_normal_median_cor.csv
* computedEmpGenes.csv
* correlation graph.pdf
* DE_genes.csv
* dz_down_sig_genes_enriched.csv
* dz_sig_used.csv
* dz_signature.csv
* dz_up_sig_genes_enriched.csv
* highExpGenes.csv
* lincs_reverse_expression.pdf
* res_geneinfo.csv
* sRGES.csv

***  

Load packages
```{r load packages, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
```
***  
Load the dataframes that contains your case and normal reference tissues.  
DO NOT VIEW these dataframes as it is very large. Use the colnames, rownames, and dim functions. Otherwise, you may crash.
```{r load files}
load(paste0(pipelineDataFolder,'dz.expr.log2.readCounts.RData')) 
#File dz.expr.log2.readCounts.demo.RData will give you mRNA counts. 
#the expression dataframe table is called dz.expr.log2.read.count.demo 

phenoDF = data.table::fread(paste0(pipelineDataFolder,'phenoDF_withage.csv'),stringsAsFactors = F) 
pam50DF = read.csv(paste0(pipelineDataFolder,'/breast_pam50_prediction.csv'),stringsAsFactors = F)
phenoDF = phenoDF %>% left_join(pam50DF,by='sample.id')
#File integrated.OCTAD.basic.sample.meta.csv consist of sample ID, cancer location, and data source, etc.  
#the phenotype dataframe is called phenoDF 

#phenoDF = phenoDF %>% filter(sample.id %in% colnames(dz.expr.log2.read.count))
#since we are dealing with demo data, there are fewer number of samples, so we need to filter out samples not within this demo.
```
***

##Explore the Phenotype data##
The phenoDF table contains thousands of samples from multiple databases. (Demo only has 5000 random samples). Run the following code to explore the phenoDF table. 
```{r explore the phenotype data}
dim(phenoDF)
head(phenoDF)
#table(phenoDF$sample.type) 
#table(phenoDF$data.source)
table(phenoDF$data.source, phenoDF$sample.type)
#table(phenoDF$cancer)
#DF stands for dataframe
```
***  

##Select Cases##    


##Select Reference Control Samples##  

***

Run DE_core_functions.R, which contains the computeRefTissue function.
```{r}
source(paste0(CodeFolder,'DE_core_functions.R'))

phenoDF.tcgaBreast = phenoDF %>% filter(data.source=='TCGA',cancer=='Breast Invasive Carcinoma',gender=='Female')

brcaptIDs = getptidDF(phenoDF.tcgaBreast$sample.id)
brcaptIDsLumA = brcaptIDs %>% left_join(pam50DF,by=c('01'='sample.id')) %>% filter(subtype=='LumA')

case_id <- as.character(brcaptIDsLumA$`01`[!is.na(brcaptIDsLumA$`01`)])
control_id = as.character(brcaptIDsLumA$`11`[!is.na(brcaptIDsLumA$`11`)])
dz_expr = dz.expr.log2.read.count[,colnames(dz.expr.log2.read.count) %in% c(case_id,control_id)]

```
***



##Visualize on Cancer Map##
This is optional.
```{r, fig.height=10, fig.align= "center"}
tsne = read.csv(paste0(pipelineDataFolder,'/tsne_2d_aeModel1.csv'), stringsAsFactors = F)

pheno_id_cancer <- phenoDF %>% select (sample.id, cancer)
tsne <- left_join (tsne, pheno_id_cancer, by = "sample.id")

#make a new column called type to state case, control, or others
tsne$type <- "others"
tsne$type[tsne$sample.id %in% case_id] <- "case"
tsne$type[tsne$sample.id %in% control_id] <- "control"

#plot 
(p2 <- ggplot(tsne, aes(X1, X2, color = type)) + geom_point(alpha = 0.4)+
    labs(title = paste ('TNSE PLOT'), x= 'TSNE Dim1', y='TSNE Dim2', caption="OCTAD"))
```

```{r}
ggsave("case_control_map.pdf", path=outputFolder)
```

##Run Differential Expressions##  
***

Variables needed:  

* case_id : samples IDs of case tissues in a character vector
* control_id : samples IDs of control tissues in a character vector
* dz_expr : dataframe that contains both case_id and control_id in its column names, 
* outputFolder : folder to place output files

***
Here, you will normalize samples for batch effects with package "RUVSeq". You can set it off by changing set parameters script.   
Set inital parameters for normalization. Below are default settings. Note down parameters that you changed from default
Keep a record of these settings if you're doing multiple runs.
You can set normalize_samples = F to make it run faster...but at risk of batch effects. 

```{r parameters}

# DE params:
normalize_samples = F # default T
#when normalize is off, k, n_topGenes are not used
k = '' # default 1
n_topGenes = '' # default 10000 for nondemo
DE_method = 'edgeR' # Default 'edgeR' ... Other choice is 'limma'

# logging parameters
write(paste0(outputFolder,'--Parameters--'), file = paste0(outputFolder,"parameters.txt"))
fileConn<-file(paste0(outputFolder,"parameters.txt"))
writeLines(c("Normalization Parameters:","normalize_samples",normalize_samples,"","k",k,"","n_topGenes",n_topGenes,"","DE_method",DE_method,"","--------------------------"), fileConn)
close(fileConn)

res = diffExp(case_id = case_id,
        control_id = control_id,
        expSet = dz_expr,
        normalize_samples = normalize_samples,
        k=k,
        n_topGenes=n_topGenes,
        DE_method='edgeR'
        )
```

***

Read csv with gene information and read Diff_Exp.R script.  
This will output 3 files: "computedEmpGenes.csv", "DE_genes.csv", and  "highExpGenes.csv".  
```{r, message=FALSE, warning=FALSE}
gene_info <- data.table::fread(paste0(pipelineDataFolder,'/gene_info_hs.csv'),stringsAsFactors = F)
ensembl_info <- data.table::fread(paste0(pipelineDataFolder,'/gencode.v23.annotation.gene.probeMap.csv'),stringsAsFactors = F)

res = res %>% left_join(ensembl_info %>% select(gene,ensembl),by=c('id'='ensembl')) %>% 
  left_join(gene_info,by=c('gene'='Symbol'))

write.csv(res,file=paste0(outputFolder,'res_geneinfo.csv'))
```
***

##Disease Signature##  
***  

Get the disease signature.
```{r}
# Dz Sig params:
padj_threshold = 0.001
log2fc_threshold = 2

#This will determine the cutoffs used for genetic signatures that is used for gene enrichment and sRGES

# logging parameters
paramFile<-paste0(outputFolder,"parameters.txt")
write(paste("Dz Sig Parameters:","padj_threshold",padj_threshold,"","log2fc_threshold",log2fc_threshold,""
             ,"--------------------------",sep = '\n'), paramFile,append = T)

dz_signature <- res %>% filter(padj < padj_threshold, abs(log2FoldChange)>log2fc_threshold)
#filter out res df to get dz sigs
#Term res refers to the results table that is generated from the Diff_Exp. R script.
dim(dz_signature)
head(dz_signature)
```
***
Generate table for disease signature
```{r}
dim(dz_signature)
head(dz_signature)
```
***

Capitalize the genes in the disease signature
```{r}
#the column name may differ between different metadata but you need Symbol to join with RGES
dz_signature$Symbol <- toupper(dz_signature$gene)
```
***

Generate "dz_signature.csv" file.
```{r}
write.csv(dz_signature, paste0(outputFolder, "/dz_signature.csv"),row.names = F)
```
***


##Affected Pathways and Potential Drugs for Reversal##  
***
Find potential drugs that may reverse disease signature.  Takes at least 10mins to run following chunk. Following scripts will output "all_lincs_score.csv"" and "sRGES.csv".  
File all_lincs_score.csv has the RGES (reversal gene expression score), which is needed for the gene_enrichment_analysis script later.    
File sREGS.csv is the summarized RGES.
```{r}
# sRGES params:
# Only mess with "max_gene_size", which is how many up-regulated genes and down-regulated genes are allowed. ("100" = 100up + 100down = 200 total)
landmark = 1 # default 1
choose_fda_drugs = F # default F
max_gene_size = 50 # default 100
weight_cell_line = F # default F

paramFile<-paste0(outputFolder,"parameters.txt")
write(paste("sRGES Parameters:","LINCS landmark genes only",landmark,"","choose FDA drugs only",choose_fda_drugs,"max gene size",max_gene_size,"weigh cell lines",weight_cell_line,""
             ,"--------------------------",sep = '\n'), paramFile,append = T)
```

```{r Run Lincs RGES}
source(paste0(CodeFolder,'drugs_core_functions.R'))

#dz_signature = read.csv('Workflow-HCC-A-2018-09-28/dz_signature.csv')
#optional load fda drugs
fda_drugs = read.csv(paste0(pipelineDataFolder,"LINCS_RGES/repurposing_drugs_20170327.csv"), comment.char = "!", header=T, sep="\t")

rges = computeLINCSrges(dz_signature,parallel = F,
                        maxGenes = max_gene_size,choose_fda = choose_fda_drugs)
write.csv(rges, paste0(outputFolder, "/all_lincs_score.csv"))
srges = summarizeLincsRGES(rges)

#still testing out the best way to compute a statistical score this is one approach
randomScores = computeRandomLincsRGES(dz_signature)

srges$p <- sapply(srges$sRGES, function(score){
      sum(randomScores < score)/length(randomScores)
    })
#srges$padj <- p.adjust(srges$p, "fdr")
write.csv(srges,paste0(outputFolder, "/sRGES.csv"))


#to get info about drugs

pred_merged_drug <- merge(fda_drugs, srges, by = "pert_iname")
pred_merged_drug <- pred_merged_drug[order(pred_merged_drug$sRGES), ]
write.csv(pred_merged_drug, paste0(outputFolder, "/sRGES_drug.csv"))
```

***
Code for parallel. However note that the output for parallel srges do not match the non-parallel version
```{r}
# source(paste0(CodeFolder,'drugs_core_functions.R'))
# 
# dz_signature = read.csv('Workflow-BRCA-LumA-Adjacent_noNorm/dz_signature.csv')
# #optional load fda drugs
# fda_drugs = read.csv(paste0(pipelineDataFolder,"LINCS_RGES/repurposing_drugs_20170327.csv"), comment.char = "!", header=T, sep="\t")
# 
# rges = computeLINCSrges(dz_signature,parallel = T,
#                         maxGenes = max_gene_size,choose_fda = choose_fda_drugs)
# write.csv(rges, paste0(outputFolder, "/all_lincs_score_par.csv"))
# srges = summarizeLincsRGES(rges)
# 
# #still testing out the best way to compute a statistical score this is one approach
# randomScores = computeRandomLincsRGES(dz_signature)
# 
# srges$p <- sapply(srges$sRGES, function(score){
#       sum(randomScores < score)/length(randomScores)
#     })
# #srges$padj <- p.adjust(srges$p, "fdr")
# write.csv(srges,paste0(outputFolder, "/sRGES_par.csv"))
# 
# 
# #to get info about drugs
# 
# pred_merged_drug <- merge(fda_drugs, srges, by = "pert_iname")
# pred_merged_drug <- pred_merged_drug[order(pred_merged_drug$sRGES), ]
# write.csv(pred_merged_drug, paste0(outputFolder, "/sRGES_drug_par.csv"))
```

See which pathways are affected by running following code. This results the up-regulated pathways as "dz_up_sig_genes_enriched.csv" and the down-regulated pathways as "dz_down_sig_genes_enriched.csv". 
This is ran on enrichR, you must be online. 
```{r Optional:enrichR}
geneEnrich(dz_signature)
#geneEnrich(dz_sigUsed,suffix='.sigGenesUsed')
```
***

Ouput the "lincs_reverse_expression.pdf' containing a heatmap showing drug hits and their effects on mRNA expression.
```{r, message=FALSE, warning=FALSE}

#pick some drugs
dz_sigUsed = read.csv(paste0(outputFolder,'/dz_sig_used.csv'))
top_drugs = as.character(srges$pert_iname[1:5])
rand_drugs = sample(as.character(srges$pert_iname[1500:1963]),replace = F,size = 5)
drugs = c(top_drugs,rand_drugs)
visualizeLincsHits(rges,dz_sigUsed,drugs)





```
***

##Drug Enrichment Analysis##
Additional files are needed here, depending on target_type.  
Each of the target_type will create a pdf and a csv file. 

* cmpd_sets_chembl_target.RData  
* cmpd_sets_mesh.RData  
* cmpd_sets_meshes.RData  
* cmpd_sets_sea_targets.RData    

The following three chunks are either both FDA-approved and not, FDA-approved only, or not FDA-approved. These chunks can take over 30min to run.

***  
1) for all results--both FDA-approved and not.  
```{r Drug Enrichment}
outputFolder = 'Workflow-BRCA-LumA-Adjacentv2/'
srges = read.csv('Workflow-BRCA-LumA-Adjacentv2/sRGES.csv')
pipelineDataFolder = '../../Data/'
source('../../Code/drugs_core_functions.R')
#targets = c('chembl_targets','mesh','meshes','sea_targets')
targets = c('mesh')

for (target_type in targets){
  load(paste0(pipelineDataFolder,"cmpd_sets_", target_type, ".RData"))
  cmpdSets = cmpd_sets$cmpd.sets
  names(cmpdSets) = cmpd_sets$cmpd.set.names
  gsea_p = drug_enrichment(sRGES=srges,target_type=target_type)
  write.csv(gsea_p, paste0(outputFolder, "/enriched_sRGES_all", target_type, ".csv"))
  for(i in 1:5){
    top_target = gsea_p$target[i]
    srges$rank = rank(srges$sRGES)
    target_drugs_scoreDF = srges[srges$pert_iname %in% cmpdSets[[top_target]],]
    write.csv(target_drugs_scoreDF,paste0(outputFolder,target_type,"_",top_target,".csv"))
    target_drugs_score = srges$rank[srges$pert_iname %in% cmpdSets[[top_target]]]
    png(filename=paste0(outputFolder,target_type,"_",top_target,".png"))
    limma::barcodeplot(srges$sRGES, target_drugs_score, main = top_target, xlab = "srges")  
    dev.off()
  }
}

#end = Sys.time() - start
```
***  

##Platform and packages used in analysis##
```{r}
library(devtools)
session_info()
```
***

End Time
```{r}
Sys.time()
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

```

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

